---
title: "Taller 1 Análisis Avanzado de Datos"
output:
  html_document: default
  pdf_document: default
date: "2024-03-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages(c("car", "glmnet", "caret"))
library(car)
library(glmnet)
library(caret)

df <- read.csv("C:/Users/Juan Diego Pulido R/Desktop/Juan Diego/2024-1/Maestría/Análisis de datos/taller1.txt")
```

### Jossie Esteban Molina P
### Juan Diego Pulido R 

1. ¿Hay multicolinealidad en los datos? Explique sucintamente.

Verificar la multicolinealidad

```{r vif_df, include=TRUE}
modelo <- lm(y ~ ., data = df)
#vif_df <- vif(modelo)
#print(vif_df)
```

El error en R al calcular el VIF (Variance Inflation Factor) indica que hay coeficientes aliased (coeficientes aliados) en el modelo de regresión. La multicolinealidad perfecta entre variables predictoras puede resultar en coeficientes aliased, lo que causa problemas en cálculos como el VIF.

2. Separe aleatoriamente (pero guarde la semilla) su conjunto de datos en dos partes

Dividir los df

```{r df_prueba, include=TRUE}
set.seed(1) # Establecer la semilla para reproducibilidad
indices <- sample(1:nrow(df), 1000)
df_entrenamiento <- df[indices, ]
df_prueba <- df[-indices, ]
```

3. Usando los 1000 datos de entrenamiento, determine los valores de λr y λl de regesión ridge y lasso, respectivamente, que minimicen el error cuadrático medio (ECM) mediante validación externa. Utilice el método de validación externa que considere más apropiado.

```{r lambda, include=TRUE}
# Determinar λr y λl
x <- model.matrix(y ~ ., data = df_entrenamiento)[,-1]
y <- df_entrenamiento$y
cv_ridge <- cv.glmnet(x, y, alpha = 0)
cv_lasso <- cv.glmnet(x, y, alpha = 1)
lambda_r <- cv_ridge$lambda.min
lambda_l <- cv_lasso$lambda.min
print(lambda_r)
print(lambda_l)
```

4. Ajuste la regresión ridge y lasso con los valores estimados de λr y λl obtenidos en (3) usando los 1000 datos de entrenamiento.

```{r modelo_lasso, include=TRUE}
# Ajustar los modelos
modelo_ridge <- glmnet(x, y, alpha = 0, lambda = lambda_r)
modelo_lasso <- glmnet(x, y, alpha = 1, lambda = lambda_l)
```

5. Para los modelos ajustados en (4) determine el más apropiado para propósitos de predicción. Considere unicamente el ECM en los 200 datos de prueba para su decisión. Seleccionar el mejor modelo

```{r ajuste, include=TRUE}
x_prueba <- model.matrix(y ~ ., data = df_prueba)[,-1]
y_prueba <- df_prueba$y
predicciones_ridge <- predict(modelo_ridge, s = lambda_r, newx = x_prueba)
predicciones_lasso <- predict(modelo_lasso, s = lambda_l, newx = x_prueba)
ECM_ridge <- mean((y_prueba - predicciones_ridge)^2)
ECM_lasso <- mean((y_prueba - predicciones_lasso)^2)
mejor_modelo <- ifelse(ECM_ridge < ECM_lasso, "Ridge", "Lasso")
```

6.Ajuste el modelo seleccionado en (5) para los 1200 datos. Note que en este punto ya tiene un λ estimado y un modelo seleccionado.

```{r modelo_datos, include=TRUE}
#Ajustar el modelo seleccionado a todos los df
x_todos <- model.matrix(y ~ ., data = df)[,-1]
y_todos <- df$y
if (mejor_modelo == "Ridge") {
  modelo_final <- glmnet(x_todos, y_todos, alpha = 0, lambda = lambda_r)
} else {
  modelo_final <- glmnet(x_todos, y_todos, alpha = 1, lambda = lambda_l)
}
print(modelo_final)
```
